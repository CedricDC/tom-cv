<html><head><title>Documentation for BoWSLAM, BaySAC, etc. parameters</title></head><body>
<h2>Documentation for BoWSLAM, BaySAC, etc. parameters</h2><h3>Generated by PARAMS.printHtmlDoc</h3><p>Add parameters to a config file, each line should read "NAME=val", e.g. "RANSAC.E_INLIER_THRESH_PX=3.5" (without quotes)</p><p>Pass path to config file as only command line argument: ./BoWSLAM /path/to/config.cfg</p><p>Image source (directory or attached cam or video file) MUST be set in config file. Calibration data is usally needed too.</p><table width='100%' border='1'>
<tr><td><b>Name</b></td><td><b>Min val</b></td><td><b>Max val</b></td><td><b>Default val</b></td><td><b>Notes</b></td></tr>
<tr><td><b></b></td><td colspan=3></td><td><b>PARAM CLASS</b> </td></tr>
<tr><td><b>BOW</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Bag-of-Words</td></tr>
<tr><td><b>BOW.BOWClustering</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Parameters for creating hierarchical dictionary (codebook/vocabulary)</td></tr>
<tr><td>BOW.BOWClustering.BF_LEVEL</td><td>0</td><td>10</td><td>3</td><td>Level of dictionary used for descriptor partitioning for BoW feature matching</td></tr>
<tr><td>BOW.BOWClustering.BRANCH_METHOD</td><td colspan=3>FixedClusterSizeTarget (FixedBF, FixedClusterSizeTarget)</td><td>How to choose centre count for sub-clusterings. Sensitivity low</td></tr>
<tr><td>BOW.BOWClustering.CLUSTER_IN_SEPERATE_THREAD</td><td>-</td><td>-</td><td>false</td><td>Whether to cluster (build a new, better, dictionary) in seperate thread. Necessary for rebuilding dictionary in RT. Probably not wanted if only building dictionary once.</td></tr>
<tr><td>BOW.BOWClustering.DESCRIPTORS_PER_WORD</td><td>5</td><td>100</td><td>15</td><td>Controls dictionary size (word count). E.g. 10k images, 300 descriptors/image, DESCRIPTORS_PER_WORD=30 would give 100k words. Sensitivity fairly low, best value unknown (effects matching and recognition differently).</td></tr>
<tr><td>BOW.BOWClustering.LEVELS</td><td>1</td><td>10</td><td>3</td><td>Levels in hierarchical dictionary</td></tr>
<tr><td>BOW.BOWClustering.RECLUSTER_FREQUENCY</td><td>-1</td><td>10</td><td>-1</td><td>If != -1, a new dictionary will be created (e.g. in background) every time number of descriptors in total grows by this factor.</td></tr>
<tr><td><b>BOW.BOWCorrespondenceProb</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Params for assigning prior probabilities to correspondences.</td></tr>
<tr><td>BOW.BOWCorrespondenceProb.MAX_PRIOR</td><td>0.01</td><td>1</td><td>0.8</td><td>Prior prob of a 1-1 correspondence. A 2-1 correspondence will have prob MAX_PRIOR/2 etc.</td></tr>
<tr><td>BOW.BOWCorrespondenceProb.MIN_PRIOR</td><td>0</td><td>1</td><td>0.4</td><td>Not used?? Prior prob of poorest 1-1 matches</td></tr>
<tr><td>BOW.COMP_METHOD</td><td colspan=3>NisterDist (NisterDist, BruteForce, VectorDistFast)</td><td>Vector distance between word bags. Brute-force compares descriptors directly (slow, no better)</td></tr>
<tr><td><b>BOW.DescriptorBinning</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Parameters for assigning descriptors to clusters</td></tr>
<tr><td>BOW.DescriptorBinning.CLUSTER_THREADS</td><td>1</td><td>64</td><td>1</td><td>MT clustering. Normally used if CLUSTER_IN_SEPERATE_THREAD=false, and waiting for new dictionary.</td></tr>
<tr><td>BOW.DescriptorBinning.LOWER_BOUND</td><td>1</td><td>64</td><td>1</td><td>Clusters with fewer than this number of descriptors not used ('stopwords'). Not very sensitive.</td></tr>
<tr><td>BOW.DescriptorBinning.RADIUS</td><td>1</td><td>[HUGE]</td><td>[HUGE]</td><td>Max radius within which descriptors will be mapped to a centre. Inferred and set based on descriptor used (PX_RADIUS) and comparison norm.</td></tr>
<tr><td>BOW.DescriptorBinning.UPPER_BOUND</td><td>1</td><td>[HUGE]</td><td>[HUGE]</td><td>Clusters bigger than UPPER_BOUND% of average size not used (indistinctive). Not very sensitive.</td></tr>
<tr><td><b>BOW.FastVectorComparison</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Approximate speeded-up query params</td></tr>
<tr><td>BOW.FastVectorComparison.SUBVEC_CACHE_SIZE</td><td>5</td><td>500</td><td>60</td><td>Todo: higher=more accurate, lower=faster, low sensitivity</td></tr>
<tr><td>BOW.FastVectorComparison.SUBVEC_LENGTH</td><td>10</td><td>200</td><td>60</td><td>Just look at first VEC_MIN_LENGTH words for first-pass match</td></tr>
<tr><td>BOW.FastVectorComparison.VEC_MIN_LENGTH</td><td>10</td><td>200</td><td>60</td><td>Just look at first VEC_MIN_LENGTH words for first-pass match</td></tr>
<tr><td>BOW.QUERY_THREADS</td><td>1</td><td>64</td><td>1</td><td>Can speed-up queries, but fast anyway.</td></tr>
<tr><td>BOW.RWB_THREADS</td><td>1</td><td>64</td><td>2</td><td>Currently blocking, so set num threads high normally.</td></tr>
<tr><td>BOW.WB_WEIGHT_METHOD</td><td colspan=3>TF_IDF (TF_IDF, DF_ITDF, TF_IDF_Wikipedia)</td><td>There's a few different TF-IDF weighting functions in the literature. Low sensitivity. Either TF_IDF, from Nister-Stewenius-2006; DF_ITDF (variation with total occurances); TF_IDF_Wikipedia (the one on Wikipedia)</td></tr>
<tr><td><b>BOWMatching</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Params for correspondences from Bag-of-Words</td></tr>
<tr><td>BOWMatching.BF_CORNER_CONDITION</td><td>0.1</td><td>1</td><td>0.8</td><td>When finding correspondences by brute-force matching, return matches within this amount of the next closest match.</td></tr>
<tr><td>BOWMatching.BF_CORRESPONDENCES</td><td colspan=3>BoWCorrespondences (BoWCorrespondences, BoW_BF_Correspondences, OldBoW_BF_Correspondences, BF_Correspondences)</td><td>Matching method: full brute-force matching of all descriptors, BF matching at course partititioning (BF_LEVEL), BoW matching (all co-occuring words give correspondences)</td></tr>
<tr><td>BOWMatching.MATCH_NN</td><td>1</td><td>12</td><td>3</td><td>Find N-M correspondences with N,M <= MATCH_NN. High (4-12) usually good.</td></tr>
<tr><td>BOWMatching.OI_NEARBY_ANGLE</td><td>1</td><td>180</td><td>30</td><td>Experimental: only match descriptors with extracted orientations within this thresh when matching nearby</td></tr>
<tr><td><b>BOWSpeedo</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Object recognition params</td></tr>
<tr><td>BOWSpeedo.DRAW_OBJECTS_AFTER_RETRAIN</td><td>-</td><td>-</td><td>false</td><td>Output all frames with best objects marked. Subject to random-access image loader (=from directory).</td></tr>
<tr><td>BOWSpeedo.MAX_PROP_COOCCURANCES</td><td>1e-06</td><td>1</td><td>0.01</td><td>Use at most this proportion of objects (only relevent for first few frames)</td></tr>
<tr><td>BOWSpeedo.NUM_OBJECTS</td><td>0</td><td>1e+06</td><td>250</td><td>Detect this many objects. 0 turns off speedo.</td></tr>
<tr><td>CORRECT_RD</td><td>-</td><td>-</td><td>true</td><td>Turn on/off radial distortion correction (set in cam calibration)</td></tr>
<tr><td><b>ConstrainScale</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Params controlling constrained scale MM</td></tr>
<tr><td>ConstrainScale.BASELINE_TO_DEPTH_LNPARAM1</td><td>0.01</td><td>100</td><td>4.4</td><td>Scale mean point depth by this LN distn. to give baseline: param 1 (mean of underlying normal)</td></tr>
<tr><td>ConstrainScale.BASELINE_TO_DEPTH_LNPARAM2</td><td>0.01</td><td>100</td><td>1.5</td><td>Scale mean point depth by this LN distn. to give baseline: param 1 (var of underlying normal)</td></tr>
<tr><td>ConstrainScale.CONSTRAIN_SCALE</td><td>-</td><td>-</td><td>false</td><td>Heuristic constraint on scale of world</td></tr>
<tr><td><b>Corner</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Corner detection params</td></tr>
<tr><td><b>Corner.CornerDetector</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Parameters for FAST and Harris-like corners</td></tr>
<tr><td>Corner.CornerDetector.CORNER_BLOCK_RAD</td><td>1</td><td>4</td><td>2</td><td>Gaussian blur size</td></tr>
<tr><td>Corner.CornerDetector.CORNER_HARRIS_K</td><td>0.003</td><td>0.1</td><td>0.01</td><td>k param for Harris corners</td></tr>
<tr><td>Corner.CornerDetector.CORNER_MIN_DIST</td><td>2</td><td>25</td><td>4</td><td>Min seperation, in pixels</td></tr>
<tr><td>Corner.CornerDetector.CORNER_MODE</td><td colspan=3>OpenCVGoodFeatures (FasterOpenCVGoodFeatures, SubSampledCorners, OpenCVGoodFeatures)</td><td>Only OpenCVGoodFeatures working properly</td></tr>
<tr><td>Corner.CornerDetector.CORNER_QUAL</td><td>0.004</td><td>0.2</td><td>0.006</td><td>Min Shi-Tomasi corner score (not usually used--top MAX_FEATURES usually selected</td></tr>
<tr><td>Corner.CornerDetector.FASTCORNER_LOCALISATION_SD</td><td>1e-06</td><td>5</td><td>0.6</td><td>Expected localisation error in pixels. 0.6 derived from test imges</td></tr>
<tr><td>Corner.CornerDetector.FASTCORNER_MIN_SEPERATION</td><td>1</td><td>64</td><td>4</td><td>5 works well, repeatability drops if worse corners let through</td></tr>
<tr><td>Corner.CornerDetector.FASTCORNER_T</td><td>1</td><td>255</td><td>21</td><td>Min strength of Harris corners (min grey level difference between centre and circle)</td></tr>
<tr><td>Corner.CornerDetector.FASTCORNER_TWO_PASS_BINNING</td><td>-</td><td>-</td><td>false</td><td>Slightly more repeatable false, true improves geometry</td></tr>
<tr><td>Corner.CornerDetector.USE_HARRIS</td><td>-</td><td>-</td><td>false</td><td>Harris corners rather than OpenCV. Hardly actually faster.</td></tr>
<tr><td>Corner.CornerDetector.USE_SUBPIX</td><td>-</td><td>-</td><td>true</td><td>Subpixel refinement. Improves localisation accuracy slightly but not to accuracy of FAST</td></tr>
<tr><td>Corner.MAX_FEATURES</td><td>10</td><td>100000</td><td>400</td><td>Max number of features detected per image. Usually reached, with top scoring corners (with min seperation) selected</td></tr>
<tr><td>Corner.SALIENT_FEATURE_TYPE</td><td colspan=3>FastCorners (SURFBlobs, ShiTomasiCorners, FastCorners)</td><td>Corner/blob type. FAST appear to be both faster and most repeatable</td></tr>
<tr><td><b>Corner.SURF</b></td><td colspan=3></td><td><b>PARAM CLASS</b> SURF blob detector (from OpenCV). Not extensively tested.</td></tr>
<tr><td>Corner.SURF.BLOB_LOCALISATION_SD</td><td>1e-06</td><td>5</td><td>0.64</td><td>SD of error in pixels. 0.64 derived from test images</td></tr>
<tr><td>Corner.SURF.CORNER_MIN_DIST</td><td>1</td><td>100</td><td>5</td><td>minimum seperation between blobs. Multiple blobs at same location never allowed</td></tr>
<tr><td>Corner.SURF.EXTENDED</td><td>-</td><td>-</td><td>false</td><td></td></tr>
<tr><td>Corner.SURF.HESSIAN_THRESH</td><td>300</td><td>500</td><td>400</td><td></td></tr>
<tr><td>Corner.SURF.OCTAVES</td><td>1</td><td>10</td><td>3</td><td></td></tr>
<tr><td>Corner.SURF.OCTAVE_LAYERS</td><td>1</td><td>10</td><td>4</td><td></td></tr>
<tr><td>DEPTH_THRESH</td><td>0.01</td><td>200</td><td>0.2</td><td>Threshhold where points become 'at infinity'. Points this far behind chosen cam also used.</td></tr>
<tr><td><b>DescriptorSetClustering</b></td><td colspan=3></td><td><b>PARAM CLASS</b> BoW dictionary generation params</td></tr>
<tr><td><b>DescriptorSetClustering.CLARA_KMedoids</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Parameters for CLARA_KMedoids clustering of descriptors</td></tr>
<tr><td>DescriptorSetClustering.CLARA_KMedoids.CLARA_ITERS</td><td>1</td><td>100</td><td>1</td><td>Cluster centres are chosen, added to a new subset, refined this many times.</td></tr>
<tr><td>DescriptorSetClustering.CLARA_KMedoids.KMEDOIDS_ITERS</td><td>1</td><td>100</td><td>2</td><td>Iterations in k-medoids alg</td></tr>
<tr><td>DescriptorSetClustering.CLARA_KMedoids.MAX_DESCRIPTORS_KM</td><td>10</td><td>1000</td><td>200</td><td>Max descriptors in each subset that is clustered. Higher=slower and slightly more accurate (cubic complexity)</td></tr>
<tr><td>DescriptorSetClustering.CLARA_KMedoids.SPARSE_ASSIGN_COUNT</td><td>1</td><td>100</td><td>1</td><td>Experimental. Speed up intermediate assignments during CLARA clustering.</td></tr>
<tr><td>DescriptorSetClustering.ClusteringAlg</td><td colspan=3>CLARA_KMedoids (CLARA_KMedoids, CLARA_KMeans, KMeans, RandomCentres, CLARA_RandomCentres)</td><td>Clustering algorithm</td></tr>
<tr><td><b>DescriptorSetClustering.KMeans</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Parameters for kmeans clustering of descriptors</td></tr>
<tr><td>DescriptorSetClustering.KMeans.KMEANS_ITERS</td><td>1</td><td>100</td><td>2</td><td>Iterations in k-means alg</td></tr>
<tr><td>DescriptorSetClustering.KMeans.SUBSET_SIZE_CLARA_KMEANS</td><td>10</td><td>10000</td><td>200</td><td>Max descriptors in each subset that is clustered. Higher=slower</td></tr>
<tr><td><b>DescriptorSetClustering.Random</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Parameters for kmeans clustering of descriptors</td></tr>
<tr><td>DescriptorSetClustering.Random.SUBSET_SIZE_CLARA_RANDOM</td><td>10</td><td>10000</td><td>200</td><td>Max descriptors in each subset that is clustered. Higher=slower</td></tr>
<tr><td>EXTRAP</td><td>-</td><td>-</td><td>false</td><td>Try extrapolating a link or 2 before giving up and starting a new map component. Constant velocity MM, uninformative scale. Sometimes helps.</td></tr>
<tr><td>FRAMERATE_MS</td><td>1</td><td>1</td><td>1</td><td>Not tested. Times==num of frames at the moment.</td></tr>
<tr><td><b>Im</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Image sizes, channels, calibration etc. Mostly set automatically.</td></tr>
<tr><td>Im.CALIBRATION_INIT</td><td>-</td><td>-</td><td>false</td><td>Flag whether calibration matrix is initialised (set automatically, not always needed)</td></tr>
<tr><td><b>Im.Camera</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Params for loading frames from an attached camera (untested)</td></tr>
<tr><td>Im.Camera.DEVICE_ID</td><td>0</td><td>1e+06</td><td>0</td><td>Select one of several attached cameras</td></tr>
<tr><td>Im.Camera.LOAD_SUBSET</td><td>1</td><td>1000</td><td>1</td><td>Only load a subset of images, e.g. 2=every other one (UNTESTED)</td></tr>
<tr><td>Im.GREYSCALE_ON_LOAD</td><td>-</td><td>-</td><td>false</td><td>Greyscale images in image loader (not good if colour frames for video needed). Turned on if ILLUMINATION_CORRECTION is set.</td></tr>
<tr><td><b>Im.Greyscale</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Params for greyscaling colour images (used for corner detection and usually for descriptors)</td></tr>
<tr><td>Im.Greyscale.B</td><td>0</td><td>1024</td><td>10</td><td>Relative importance of blue channel, default values work well experimentally</td></tr>
<tr><td>Im.Greyscale.G</td><td>0</td><td>1024</td><td>150</td><td>Relative importance of green channel, default values work well experimentally</td></tr>
<tr><td>Im.Greyscale.GAMMA</td><td>0</td><td>5</td><td>1</td><td>Gamma correction, 1 turns it off (works best)</td></tr>
<tr><td>Im.Greyscale.R</td><td>0</td><td>1024</td><td>250</td><td>Relative importance of red channel, default values work well experimentally</td></tr>
<tr><td>Im.ILLUMINATION_CORRECTION</td><td colspan=3>NoIlluminationCorrection (NoIlluminationCorrection, EqualiseHist, EqualiseMean, EqualiseMeanSD)</td><td>Correction--can also set SAVE_FRAMES to save correced frames for next run</td></tr>
<tr><td>Im.IM_CHANNELS</td><td>1</td><td>3</td><td>3</td><td>Set automatically by image loader. Num channels in images being loaded. 1 (grey) and 3 (RGB) supported in Linux, possibly only 3 in Windows.</td></tr>
<tr><td>Im.IM_HEIGHT</td><td>32</td><td>2184</td><td>480</td><td>Set automatically by image loader. NB this has to fit in a short (allowing fast detection of duplicates), limiting max val. #define SUBPIX_RES to something lower to work with large images (see PX_SCALE_INT=SUBPIX_RES in location.h).</td></tr>
<tr><td>Im.IM_SOURCE</td><td colspan=3>ImageDir (ImageSim, ImageDir, SimMap, VideoFile, Camera)</td><td>Frame source</td></tr>
<tr><td>Im.IM_WIDTH</td><td>32</td><td>2184</td><td>640</td><td>Set automatically by image loader. NB this has to fit in a short (allowing fast detection of duplicates), limiting max val. #define SUBPIX_RES to something lower to work with large images (see PX_SCALE_INT=SUBPIX_RES in location.h).</td></tr>
<tr><td><b>Im.ImageDir</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Params for loading frames from images from a directory</td></tr>
<tr><td>Im.ImageDir.IMAGE_DIR</td><td colspan=3></td><td>Path to folder containing images (jpg, png, bmp, pgm, tiff if HAVE_TIFF defined)</td></tr>
<tr><td>Im.ImageDir.LOAD_SUBSET</td><td>1</td><td>1000</td><td>1</td><td>Only load a subset of images, e.g. 2=every other one</td></tr>
<tr><td>Im.RD2</td><td>-10</td><td>10</td><td>0</td><td>RD correction coeffs. For tuning. Don't use, set in config file instead.</td></tr>
<tr><td>Im.RD4</td><td>-10</td><td>10</td><td>0</td><td>RD correction coeffs. For tuning. Don't use, set in config file instead.</td></tr>
<tr><td>Im.SAVE_FRAMES</td><td colspan=3>DontSave (DontSave, SaveBMP, SavePNG, SaveJPG)</td><td>Dumps all frames used to 'frames' folder, *includes illumination corrections, resizeing, etc.</td></tr>
<tr><td>Im.SCALE_DOWN</td><td>1</td><td>10</td><td>1</td><td>Scale down images in image loader by this factor</td></tr>
<tr><td><b>Im.VideoFile</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Params for loading frames from a video file (.avi usually works)</td></tr>
<tr><td>Im.VideoFile.FILENAME</td><td colspan=3></td><td>Filename and path to video file. Make sure calib.txt containing calibration is in the same folder.</td></tr>
<tr><td>Im.VideoFile.LOAD_SUBSET</td><td>1</td><td>1000</td><td>1</td><td>Only load a subset of images, e.g. 2=every other one</td></tr>
<tr><td><b>LinkSelection</b></td><td colspan=3></td><td><b>PARAM CLASS</b> How many frames to link to, etc.</td></tr>
<tr><td>LinkSelection.ALLOW_ZERO_VELOCITY_LINKS</td><td>-</td><td>-</td><td>false</td><td>Add links where camera hasn't moved significantly but has rotated. If added then scale can not be propogated through them. If not added then pure rotation is handled if this frame can be registered to another earlier frame.</td></tr>
<tr><td>LinkSelection.DEBUG_DISABLE_LINKING</td><td>-</td><td>-</td><td>false</td><td>For BoW matching debugging</td></tr>
<tr><td>LinkSelection.KEEP_ALL_LINKS</td><td>-</td><td>-</td><td>true</td><td>If true then only frames registered successfully to no others will be dropped. False helps keep map sparse, but misses rotations</td></tr>
<tr><td>LinkSelection.MAX_NUM_TO_TRY_LINKING</td><td>1</td><td>16</td><td>4</td><td>Max number of topologically nearby frames to *attempt to* link to the current frame</td></tr>
<tr><td>LinkSelection.MAX_NUM_TO_TRY_LINKING_LC</td><td>0</td><td>16</td><td>2</td><td>Max number of distant frames to *attempt to* link to the current frame (loop closure)</td></tr>
<tr><td>LinkSelection.NUM_TO_LINK</td><td>1</td><td>16</td><td>3</td><td>Max number of topologically nearby frames to link to the current frame</td></tr>
<tr><td>LinkSelection.NUM_TO_LINK_LC</td><td>0</td><td>16</td><td>1</td><td>Max number of distant frames to link to the current frame (loop closure)</td></tr>
<tr><td>LinkSelection.SIMILARITY_THRESH</td><td>0</td><td>1</td><td>0.85</td><td>Threshhold on distinctiveness needed to be considered LC candidate--lower val will mean more matches will meet threhhold to be considered LC candidates</td></tr>
<tr><td>LinkSelection.VERBOSE</td><td>-</td><td>-</td><td>false</td><td>List strongest BoW matches</td></tr>
<tr><td>MAX_TRACK_LEN</td><td>10</td><td>10000</td><td>10000</td><td>Delete me. If re-implemented us BoW matching</td></tr>
<tr><td>MAX_TRIES_FINDING_E</td><td>1</td><td>5</td><td>1</td><td>Give RANSAC a second go at finding E if the first attempt doesn't lead to a camera being selected</td></tr>
<tr><td>MIN_INLIERS_DISTANT</td><td>8</td><td>100</td><td>28</td><td>Matches with far away frames (loop closure) need this many inlier correspondences</td></tr>
<tr><td>MIN_INLIERS_NEARBY</td><td>8</td><td>50</td><td>20</td><td>Matches with nearby frames need this many inlier correspondences</td></tr>
<tr><td>MIN_TRACK_LEN_RECONSTRUCT</td><td>0</td><td>16</td><td>2</td><td>Not implemented (didn't help)</td></tr>
<tr><td>MULTI_RUNS</td><td>-</td><td>-</td><td>false</td><td>Internal flag turning off output--tuning or something is happening</td></tr>
<tr><td><b>Mapping</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Params controlling local-global map generation</td></tr>
<tr><td>Mapping.SET_ORIGIN</td><td colspan=3>AtZero (AtZero, AtRobot)</td><td>Generate either a map relative to the robot's current position, or one realtive to the first frame. Affects where gaps form. Can be moved around in code.</td></tr>
<tr><td>Mapping.VERBOSE</td><td>-</td><td>-</td><td>false</td><td>Dijkstra-update output</td></tr>
<tr><td>NEARBY_TIME</td><td>0</td><td>250</td><td>2</td><td>Frames within this many timesteps are defined as nearby. See RECURSE_DEPTH.</td></tr>
<tr><td><b>Optimise</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Params controlling optimisation around loops (in t-spanner)</td></tr>
<tr><td>Optimise.SPANNER_T</td><td>2</td><td>10000</td><td>50</td><td>t-Spanner t for subgraph used to optimise scale around loops</td></tr>
<tr><td>Optimise.VERBOSE</td><td>-</td><td>-</td><td>false</td><td>Print SVD matrix, corrections, etc.</td></tr>
<tr><td><b>Output</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Params for controlling rendering frequency, EPS output, etc. Rendering is more expensive than everything else combined!</td></tr>
<tr><td>Output.KILL_FRAME</td><td>0</td><td>[HUGE]</td><td>[HUGE]</td><td>Force segfault, use with Valgrind to track mem in use</td></tr>
<tr><td>Output.OPTIMISE_SCALES</td><td>-</td><td>-</td><td>false</td><td>Use SVD to optimise scales around loops</td></tr>
<tr><td>Output.OUTPUT_CORR</td><td>-</td><td>-</td><td>false</td><td>Save images showing inlier and outlier correspondences after doing RANSAC</td></tr>
<tr><td>Output.PLOT_INTERVAL</td><td>1</td><td>10000</td><td>1</td><td>Render+show every PLOT_INTERVAL'th frame</td></tr>
<tr><td>Output.PRINT_FRAME_NUMS</td><td>0</td><td>10000</td><td>10</td><td>Display every PRINT_FRAME_NUMS'th frame number</td></tr>
<tr><td>Output.PRINT_HEURISTIC_ERRORS</td><td>-</td><td>-</td><td>false</td><td>Print warnings when the camera is stationary, fast, moving backwards or rotating out of XZ plane (use to identify failure points)</td></tr>
<tr><td>Output.PRINT_POSITION_SOURCES</td><td>-</td><td>-</td><td>false</td><td>Display link (yellow line) to most recent frame each frame is registered to</td></tr>
<tr><td>Output.PRINT_POS_SOURCES</td><td>-</td><td>-</td><td>false</td><td>VERY SLOW: Print the sequence of relative poses and scales added to compute each position. Produces a huge amount of output!</td></tr>
<tr><td>Output.PRINT_SPEEDS</td><td>-</td><td>-</td><td>false</td><td>Print the sequences of edges, and transforms, used to compute</td></tr>
<tr><td>Output.SAVE_EPS</td><td>-</td><td>-</td><td>true</td><td>Save local map as EPS</td></tr>
<tr><td>Output.TEST_RD_CORRECTION</td><td>-1</td><td>[HUGE]</td><td>-1</td><td>Use Radial distortion parameters to rectify the TEST_RD_CORRECTION'th frame. Output saved in output dir.</td></tr>
<tr><td><b>PatchDescriptor</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Image patch selection params</td></tr>
<tr><td>PatchDescriptor.PATCH_COMP_METHOD</td><td colspan=3>PatchEuclidFast (PatchEuclidFast, PatchL1Fast, PatchMaxDist, PatchCorrel, PatchL1, PatchEuclid, PatchEuclidParallel, PatchL1Parallel)</td><td>Choose norm for comparing patches. Euclidean seems best. The squared val is used.</td></tr>
<tr><td>PatchDescriptor.PATCH_RAD</td><td>1</td><td>6</td><td>5</td><td>Patch will be square with sides 2*PATCH_RAD+1</td></tr>
<tr><td>PatchDescriptor.PX_RADIUS</td><td>1</td><td>255</td><td>50</td><td>If 2 descriptors have average differnce of more than this many grey levels they are different.</td></tr>
<tr><td><b>PatchDescriptor.Patch</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Simplest descriptor: Just an image patch centred around corner</td></tr>
<tr><td>PatchDescriptor.Patch.MONO_DESCRIPTOR</td><td>-</td><td>-</td><td>true</td><td>Greyscale rather than RGB descriptors</td></tr>
<tr><td>PatchDescriptor.Patch.NORMALISE</td><td>-</td><td>-</td><td>false</td><td>Normalise intensity in each descriptor (so each has the same mean; for illumination invariance). Normalising image intensity probably works better.</td></tr>
<tr><td>PatchDescriptor.Patch.ORIENT</td><td>-</td><td>-</td><td>false</td><td>Rotation invariance: orient with strongest change in intensity (a bit like SURF).</td></tr>
<tr><td>PatchDescriptor.Patch.PATCH_BLUR</td><td>0</td><td>10</td><td>2</td><td>A Gaussian blur can be applied to whole image (if > 0) to get rid of noise effects when subsampling.</td></tr>
<tr><td>PatchDescriptor.Patch.PATCH_SCALE</td><td>1</td><td>10</td><td>4</td><td>Patches are subsampled. Essentially the same as downsizing input images (except corner localisation will be better). Don't know what is best</td></tr>
<tr><td><b>RANSAC</b></td><td colspan=3></td><td><b>PARAM CLASS</b> RANSAC for E params</td></tr>
<tr><td>RANSAC.E_8PT_CUTOFF</td><td>1</td><td>1e+06</td><td>5</td><td>Reject essential matrix candidates from 8pt algorithm where the singular values of F differ by this factor (they would be equal if F was a good estimate for E).</td></tr>
<tr><td>RANSAC.E_INLIER_THRESH_PX</td><td>0.001</td><td>[HUGE]</td><td>3</td><td>E inlier threshhold in pixels. Focal length from camera calibration matrix used to translate to image coordinates. Can be set huge to change RANSAC to a simple least-squares fit (a bit hacky)</td></tr>
<tr><td>RANSAC.HypothesiseAlg</td><td colspan=3>5PtE (5PtE, 7PtE, 7PtF, 2PtE, MCE, 1PtE, 5Pt_GradientDesc, 4Pt_GradientDesc, 3Pt_GradientDesc, 7PtEFast, 5PtRt)</td><td>Choose a hypothesis-generation algorithm, also chooses whether to find Essential or Fundamental matrix</td></tr>
<tr><td>RANSAC.MAX_ITERS</td><td>1</td><td>1e+06</td><td>250</td><td>Max number of RANSAC iterations (number of hypothesis sets chosen, may be multiple models per hyp. set). Normally fewer iterations needed</td></tr>
<tr><td>RANSAC.PROB_SUCCESS</td><td>0.8</td><td>0.999</td><td>0.98</td><td>RANSAC parameter; terminate when this likely to have found inliers.</td></tr>
<tr><td>RANSAC.RANSACIterTerminator</td><td colspan=3>ClassicRANSAC (MaxIters, ClassicRANSAC, TerminateOnPropInliers)</td><td>Algorithm to decide when to stop iterating (because an adequate solution has been found). F+B's paper includes a simple formula (ClassicRANSAC), otherwise terminate only after MAX_ITERS</td></tr>
<tr><td>RANSAC.RANSACSampler</td><td colspan=3>BaySAC (RANSAC, BaySAC, SimSAC)</td><td>Sampling method</td></tr>
<tr><td>RANSAC.RANSACTerminator</td><td colspan=3>SimpleTerminator (NoTestTerminator, SimpleTerminator, WaldSAC, BrownianBridge, BrownianBridgeLinear)</td><td>Algorithm to decide when to give up testing data points (when the inlier rate is low). WaldSAC-based terminators are too slow, NoTestTerminator or SimpleTerminator should be used.</td></tr>
<tr><td>RANSAC.TERMINATOR_LOG</td><td>-</td><td>-</td><td>false</td><td>Output data on when the 'test' stage terminates (e.g. when using WaldSAC)</td></tr>
<tr><td>RANSAC.TOPDOWN_EXPAND</td><td>0.2</td><td>10</td><td>1</td><td>Experimental: Scale up inlier threshhold when finding additional inliers (topdown refinement)</td></tr>
<tr><td>RANSAC.TOPDOWN_ITERS</td><td>0</td><td>1000</td><td>3</td><td>Remove outliers, then re-fit model to remaining inliers this many times.</td></tr>
<tr><td>RANSAC.TOPDOWN_SCALEDOWN</td><td>0.1</td><td>1.2</td><td>1</td><td>Experimental: Reduce inlier threshhold for each topdown refinement iteration</td></tr>
<tr><td>RANSAC.VERBOSE</td><td>-</td><td>-</td><td>false</td><td>Output debugging info to cout</td></tr>
<tr><td>READ_AHEAD_LIM</td><td>0</td><td>1e+06</td><td>3</td><td>How far ahead can the image-loading thread get. SET LOW=0 FOR OUTPUTTING VIDEO FRAMES</td></tr>
<tr><td>RECURSE_DEPTH</td><td>1</td><td>25</td><td>2</td><td>Frames successfully matched (linked) this deeply to the NEARBY_TIME frames are defined to be nearby. Works well even at variable speed because of multiple links.</td></tr>
<tr><td><b>RefineRT</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Levenberg-Marquardt refinement of rotation and translation params.</td></tr>
<tr><td>RefineRT.ROBUST_COST</td><td>-</td><td>-</td><td>false</td><td>Can use a robust cost fn where  we discount cost above a threshhold. Doesn't seem to help</td></tr>
<tr><td>RefineRT.ROBUST_COST_CONDITION_SCALE</td><td>0</td><td>1000</td><td>5</td><td>If av residual too high then boost G_sq so unlikely to be used.</td></tr>
<tr><td>RefineRT.ROBUST_COST_CONDITION_THRESH</td><td>0</td><td>1</td><td>2e-05</td><td>If av residual exceeds this amount then outliers probably included. TODO: Try outlier removal</td></tr>
<tr><td>RefineRT.ROBUST_COST_SCALE</td><td>0</td><td>1</td><td>0.1</td><td>Scale down part of residual greater than thresh by this amount</td></tr>
<tr><td>RefineRT.ROBUST_COST_THRESH</td><td>0</td><td>1</td><td>0.01</td><td>Thresh above which cost is discounted (for robust cost function)</td></tr>
<tr><td>RefineRT.VERBOSE</td><td>-</td><td>-</td><td>false</td><td>Output condition data</td></tr>
<tr><td><b>ResolveScale</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Params for estimating scale from 3d point matches; Grubs test, etc.</td></tr>
<tr><td>ResolveScale.ALIGNMENT_THETA_MAX_ERR</td><td>0.01</td><td>0.9</td><td>0.15</td><td>Matches between 3D points considered outliers if the angle between points (in the same frame) differ by more than this amount. Would be 0 in noise-free case.</td></tr>
<tr><td>ResolveScale.MIN_INLIERS_1DALIGN</td><td>3</td><td>1000</td><td>3</td><td>Minimum number of matches between 3D point sets for scale estimate to be used</td></tr>
<tr><td>ResolveScale.MOTION_MODEL</td><td>-</td><td>-</td><td>false</td><td>Can turn on a simple constant-velocity motion model, but doesn't seem to help even with auto parameter tuning.</td></tr>
<tr><td>ResolveScale.MOTION_MODEL_MAX_TIME</td><td>2</td><td>1000</td><td>10</td><td>Links from this long ago are used to estimate future motion</td></tr>
<tr><td>ResolveScale.MOTION_MODEL_SD</td><td>1e-05</td><td>1000</td><td>0.1</td><td>SD of future position estimate as a proportion of current velocity</td></tr>
<tr><td>ResolveScale.PROB_NO_MOTION_MODEL</td><td>0</td><td>1</td><td>0.05</td><td>Probability of any position estimate incompatible with MM occurring (so essentially uniform+Gaussian MM)</td></tr>
<tr><td>ResolveScale.VERBOSE</td><td>-</td><td>-</td><td>false</td><td>MM Debugging output</td></tr>
<tr><td>START_FRAME</td><td>0</td><td>[HUGE]</td><td>0</td><td>Delay navigation until this many frames are observed. Use for debugging to get BoW DB initialised or to start somewhere easier.</td></tr>
<tr><td>TARGET_CORRESPONDENCES</td><td>0</td><td>10000</td><td>500</td><td>Not implemented</td></tr>
<tr><td><b>TORO</b></td><td colspan=3></td><td><b>PARAM CLASS</b> Params for outputting optimised TORO files</td></tr>
<tr><td>TORO.EXTRA_UNINF_EDGES</td><td>-</td><td>-</td><td>false</td><td>Can impose a Motion model by adding extra edges to TORO map. TODO: Investigate further for removing artifacts.</td></tr>
<tr><td>TORO.SAVE_TORO_MAP</td><td>-</td><td>-</td><td>true</td><td>Save a file for input to TORO</td></tr>
<tr><td>TORO.TORO_CONNECTIVITY</td><td>1</td><td>10000</td><td>5</td><td>t-Spanner t value for TORO map output</td></tr>
<tr><td>TORO.TWO_D</td><td>-</td><td>-</td><td>false</td><td>Can output a 2d (X-Z) map to TORO instead--often works better. Full covariances could be used but aren't computed at the moment</td></tr>
<tr><td>TOTAL_CORES</td><td>1</td><td>64</td><td>2</td><td>Limits number of threads spawned in some circumstances (multithreaded RANSAC and calculating several links at once). Debug build defaults to lower val.</td></tr>
<tr><td>TUNE_PARAMS</td><td>-</td><td>-</td><td>false</td><td>Internal flag turning off output for tuning</td></tr>
</table>
</body></html>